{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Artificial Neural Networks.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural networks\n",
        "- Let us see the differences between neural networks which apply ReLU and those which do not apply ReLU\n",
        "\n",
        "\n",
        "## 순서\n",
        "1. Calculate the first and second hidden layer by multiplying the appropriate inputs with the corresponding weights.\n",
        "2. Calculate and print the results of the output.\n",
        "3. Set weight_composed_1 to the product of weight_1 with weight_2, then set weight to the product of weight_composed_1 with weight_3.\n",
        "4. Calculate and print the output.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rTqjcmsE3h6f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YMulylwxUCJ"
      },
      "outputs": [],
      "source": [
        "# Calculate the first and second hidden layer\n",
        "hidden_1 = torch.matmul(input_layer, weight_1)\n",
        "hidden_2 = torch.matmul(hidden_1, weight_2)\n",
        "\n",
        "# Calculate the output\n",
        "print(torch.matmul(hidden_2, weight_3))\n",
        "\n",
        "# Calculate weight_composed_1 and weight\n",
        "weight_composed_1 = torch.matmul(weight_1, weight_2)\n",
        "weight = torch.matmul(weight_composed_1, weight_3)\n",
        "\n",
        "# Multiply input_layer with weight\n",
        "print(torch.matmul(input_layer, weight))\n",
        "\n",
        "# result\n",
        "# tensor([[0.2655, 0.1311, 3.8221, 3.0032]])\n",
        "# tensor([[0.2655, 0.1311, 3.8221, 3.0032]])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ReLU activation"
      ],
      "metadata": {
        "id": "eoq2IybC4E3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate non-linearity\n",
        "relu = nn.ReLU()\n",
        "\n",
        "# Apply non-linearity on the hidden layers\n",
        "hidden_1_activated = relu(torch.matmul(input_layer, weight_1))\n",
        "hidden_2_activated = relu(torch.matmul(hidden_1_activated, weight_2))\n",
        "print(torch.matmul(hidden_2_activated, weight_3))\n",
        "\n",
        "# Apply non-linearity in the product of first two weights. \n",
        "weight_composed_1_activated = relu(torch.matmul(weight_1, weight_2))\n",
        "\n",
        "# Multiply `weight_composed_1_activated` with `weight_3\n",
        "weight = torch.matmul(weight_composed_1_activated, weight_3)\n",
        "\n",
        "# Multiply input_layer with weight\n",
        "print(torch.matmul(input_layer, weight))\n",
        "\n",
        "# result\n",
        "# tensor([[-0.2770, -0.0345, -0.1410, -0.0664]])\n",
        "# tensor([[-0.2115, -0.4782,  4.0437,  3.0415]])"
      ],
      "metadata": {
        "id": "MLdlHAPw481t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ReLU activation again"
      ],
      "metadata": {
        "id": "JPcCZR4u4-A7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate ReLU activation function as relu\n",
        "relu = nn.ReLU()\n",
        "\n",
        "# Initialize weight_1 and weight_2 with random numbers\n",
        "weight_1 = torch.rand(4, 6)\n",
        "weight_2 = torch.rand(6, 2)\n",
        "\n",
        "# Multiply input_layer with weight_1\n",
        "hidden_1 = torch.matmul(input_layer, weight_1)\n",
        "\n",
        "# Apply ReLU activation function over hidden_1 and multiply with weight_2\n",
        "hidden_1_activated = relu(hidden_1)\n",
        "print(torch.matmul(hidden_1_activated, weight_2))"
      ],
      "metadata": {
        "id": "8QucpVI-6dXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss functions\n",
        "- 각 항을 지수화하여(e를 각 점수의 거듭제곱으로 설정), 합이 1이 아닌 숫자인 비정규화 확률을 제공합니다.\n",
        "- 그런 다음 각 항을 모든 항의 합으로 나눈 분모에서, 예를 들어 비정규화 확률의 합은 188.68이므로 24.5입니다.\n",
        "- 마지막으로 교차 엔트로피 손실을 계산합니다. 올바른 클래스의 확률에 대한 마이너스 로그(밑수 e 포함)입니다. -ln(0.13)이 2.0404임을 스스로 계산할 수 있습니다. 주의할 점: 올바른 클래스의 확률이 1이고 손실이 0이면 네트워크가 완벽하게 작동하는 것입니다. 확률이 0에 가까우면 손실이 크고 무한할 수 있으며 예측이 빗나가게 됩니다."
      ],
      "metadata": {
        "id": "wzBidW0E6eJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the scores and ground truth\n",
        "logits = torch.tensor([[-1.2, 0.12, 4.8]])\n",
        "ground_truth = torch.tensor([2])\n",
        "\n",
        "# Instantiate cross entropy loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Compute and print the loss\n",
        "loss = criterion(logits, ground_truth)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "BQqpcMj2-lt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import torch and torch.nn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Initialize logits and ground truth\n",
        "logits = torch.rand(1, 1000)\n",
        "ground_truth = torch.tensor([111])\n",
        "\n",
        "# Instantiate cross-entropy loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Calculate and print the loss\n",
        "loss = criterion(logits, ground_truth)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "HSszXDlN_TT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing a dataset in PyTorch\n",
        "- torchvision (a package which deals with datasets and pretrained neural nets) \n",
        "- torch.utils.data. From torchvision submodule"
      ],
      "metadata": {
        "id": "_r3YJvNJBOwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform the data to torch tensors and normalize it to have mean is 0.1307 and std is 0.3081.\n",
        "# Prepare the trainset and the testset.\n",
        "# Prepare the dataloaders for training and testing so that only 32 pictures are processed at a time and the training data is shuffled each time.\n",
        "\n",
        "# Transform the data to torch tensors and normalize it \n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "\t\t\t\t\t\t\t\ttransforms.Normalize((0.1307), ((0.3081)))])\n",
        "\n",
        "# Prepare training set and testing set\n",
        "trainset = torchvision.datasets.MNIST('mnist', train=True, \n",
        "\t\t\t\t\t\t\t\t\t  download=True, transform=transform)\n",
        "testset = torchvision.datasets.MNIST('mnist', train=False,\n",
        "\t\t\t   download=True, transform=transform)\n",
        "\n",
        "# Prepare training loader and testing loader\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
        "\t\t\t\t\t\t\t\t\t\t shuffle=False, num_workers=0) "
      ],
      "metadata": {
        "id": "UPRWAPu5Cc0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the shape of the training set and testing set\n",
        "trainset_shape = trainloader.dataset.train_data.shape\n",
        "testset_shape = testloader.dataset.test_data.shape\n",
        "\n",
        "# Print the computed shapes\n",
        "print(trainset_shape, testset_shape)\n",
        "\n",
        "# Compute the size of the minibatch for training set and testing set\n",
        "trainset_batchsize = trainloader.batch_size\n",
        "testset_batchsize = testloader.batch_size\n",
        "\n",
        "# Print sizes of the minibatch\n",
        "print(trainset_batchsize, testset_batchsize)"
      ],
      "metadata": {
        "id": "imSMjIHSDWmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training neural networks\n",
        "## Recipe for training neural network\n",
        "1. Prepare the data loaders\n",
        "2.  Build a neural network\n",
        "Loop over\n",
        "- do a forward pass\n",
        "- calculate loss function\n",
        "-  calculate loss gradients\n",
        "- change the weights based on gradients "
      ],
      "metadata": {
        "id": "ILrnPFfKDVfY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the class Net\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):    \n",
        "    \t# Define all the parameters of the net\n",
        "        super(Net, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28 * 1, 200)\n",
        "        self.fc2 = nn.Linear(200, 10)\n",
        "\n",
        "    def forward(self, x):   \n",
        "    \t# Do the forward pass\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "yiGDmHpaFm3P"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the fully connected neural network (called model) which you built in the previous exercise and a train loader called train_loader containing the MNIST dataset (which we created for you), you're to train the net in order to predict the classes of digits. You will use the Adam optimizer to optimize the network, and considering that this is a classification problem you are going to use cross entropy as loss function."
      ],
      "metadata": {
        "id": "6uPGCcoaGt2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the Adam optimizer and Cross-Entropy loss function\n",
        "model = Net()   \n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "  \n",
        "for batch_idx, data_target in enumerate(train_loader):\n",
        "    data = data_target[0] # input\n",
        "    target = data_target[1] # labels\n",
        "    data = data.view(-1, 28 * 28)\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward, backward, optimize\n",
        "    # Complete a forward pass\n",
        "    output = model(data)\n",
        "\n",
        "    # Compute the loss, gradients and change the weights\n",
        "    loss = criterion(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "id": "_VUp2sQbFoLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model in eval mode\n",
        "model.eval()\n",
        "\n",
        "for i, data in enumerate(test_loader, 0):\n",
        "    inputs, labels = data\n",
        "    \n",
        "    # Put each image into a vector\n",
        "    inputs = inputs.view(-1, 28*28)\n",
        "    \n",
        "    # Do the forward pass and get the predictions\n",
        "    outputs = model(inputs)\n",
        "    _, outputs = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (outputs == labels).sum().item()\n",
        "print('The testing set accuracy of the network is: %d %%' % (100 * correct / total))"
      ],
      "metadata": {
        "id": "O5RQ98hRHeMv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}